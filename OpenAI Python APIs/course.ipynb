{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dotenv_values('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = config['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7lXobOwIQVsHNiyw1b65OTotT60Ad at 0x2a96d8bd9e8> JSON: {\n",
       "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
       "  \"id\": \"cmpl-7lXobOwIQVsHNiyw1b65OTotT60Ad\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1691565757,\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"\\n\\n1. Tokyo, Japan\\n2. Delhi, India\\n3. Shanghai, China \\n4. Sao Paulo, Brazil\\n5. Mexico City, Mexico\\n6. Cairo, Egypt\\n7. Beijing, China\\n8. Dhaka, Bangladesh\\n9. Mumbai, India\\n10. Osaka, Japan\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 9,\n",
       "    \"completion_tokens\": 65,\n",
       "    \"total_tokens\": 74\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.Completion.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt='The top 10 most populated cities are: ', \n",
    "    max_tokens=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt='Generate a list of best movies of all time', \n",
    "    max_tokens=100, \n",
    "    stop='5.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. The Godfather (1972)\n",
      "2. Schindlerâ€™s List (1993) \n",
      "3. Casablanca (1942)\n",
      "4. The Shawshank Redemption (1994)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7lZA4JwBjJ9Y23VQSsLWJAXo0jfQH at 0x2a96da60830> JSON: {\n",
       "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
       "  \"id\": \"cmpl-7lZA4JwBjJ9Y23VQSsLWJAXo0jfQH\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1691570932,\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"\\n\\nQ: What did the necrophiliac say when he was released from jail? \\nA: \\\"I'm free at last!\\\"\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    },\n",
       "    {\n",
       "      \"text\": \"\\n\\nQ: What did the Death say to the two flies crisscrossing in the air?\\nA: Stop buzzing around, you're going to give me a complex!\",\n",
       "      \"index\": 1,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    },\n",
       "    {\n",
       "      \"text\": \"\\n\\nQ: Why don't skeletons ever go out on the town?\\nA: Because they have no body to go with.\",\n",
       "      \"index\": 2,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 6,\n",
       "    \"completion_tokens\": 94,\n",
       "    \"total_tokens\": 100\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.Completion.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt='Tell me a joke about death', \n",
    "    max_tokens=100, \n",
    "    n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7lZIKB22t6uaBh5B55b9D85azIbJh at 0x2a96dab65c8> JSON: {\n",
       "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
       "  \"id\": \"cmpl-7lZIKB22t6uaBh5B55b9D85azIbJh\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1691571444,\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"Tell me a joke about death\\n\\nQ: What did the vampire call Death?\\nA: His \\u201cdearly departed\\u201d friend!\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 6,\n",
       "    \"completion_tokens\": 24,\n",
       "    \"total_tokens\": 30\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.Completion.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt='Tell me a joke about death', \n",
    "    max_tokens=100, \n",
    "    echo=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Your article was so fabulous; I especially appreciate the further insights you have drawn to enracinate our outlook. :)\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt='Tell me sth nice', \n",
    "    max_tokens=50, \n",
    "    temperature=1.5\n",
    ")\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top P sets the limit for cumulative probability for the next token. It restricts the candidate words to the smallest set whose cumulative probability is >= to a given threshold P. With top p set to one, it can select any token in the model. The top P parameter does not change the probabilities of the world occuring, but changes the pool.\n",
    "\n",
    "In contrast, temperature is actually changing the probabilities, increasing low probability worlds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You are an amazing person and you have so much to offer the world.\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt='Tell me sth nice', \n",
    "    max_tokens=50, \n",
    "    top_p=0\n",
    ")\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency penalty penalizes new tokens based on existing frequency. -2 encourages reoccurence, 2, prevents. Decreases likelihood to repeat the same line verbatim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You are an amazing person with a lot of love to give.\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt='Tell me sth nice', \n",
    "    max_tokens=50, \n",
    "    frequency_penalty=2\n",
    ")\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presence penalty penalizes new tokes whether they appear in the text so far. This is one-off addtitive contribution, while frequency penalty is proportional, so the more token appeared, the more it is penalized.  Increases likelihood that the model will talk about new topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strem equal to True allows to output a generator. This way the API will not wait untill the output is generated, but stream it live. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You have done an amazing job!"
     ]
    }
   ],
   "source": [
    "for data in openai.Completion.create(\n",
    "    model='text-davinci-003',\n",
    "    prompt='Tell me sth nice', \n",
    "    max_tokens=50, \n",
    "    stream=True\n",
    "):\n",
    "    print(data.choices[0].text, end = \"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT endpoit\n",
    "! it is 10x cheaper to use gpt-3.5-turbo than Da Vinci model, although it is much better and newer. But gpt-3.5-turbo is only available via ChatCompletion endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7mGb23b3EKhM8PtRitHQHYXFAYGY0 at 0x2a96db0b258> JSON: {\n",
       "  \"id\": \"chatcmpl-7mGb23b3EKhM8PtRitHQHYXFAYGY0\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1691737896,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Sure! Here are three trivial questions and their answers:\\n\\n1. What is the capital city of France?\\n   Answer: Paris\\n\\n2. What is the symbol for the chemical element sodium?\\n   Answer: Na\\n\\n3. Who painted the Mona Lisa?\\n   Answer: Leonardo da Vinci\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 24,\n",
       "    \"completion_tokens\": 57,\n",
       "    \"total_tokens\": 81\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo', \n",
    "    messages=[\n",
    "        {'role':'system', 'content': 'You are a helpful assistant'},\n",
    "        {'role': 'user', 'content': 'Give me 3 trivial questions and answers'}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7mGhk5UwbpjuLpMpxez8fJukgj5ca at 0x2a96db0beb8> JSON: {\n",
       "  \"id\": \"chatcmpl-7mGhk5UwbpjuLpMpxez8fJukgj5ca\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1691738312,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Negative\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 55,\n",
       "    \"completion_tokens\": 1,\n",
       "    \"total_tokens\": 56\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.ChatCompletion.create(\n",
    "    model='gpt-3.5-turbo', \n",
    "    messages=[\n",
    "        {'role':'system', 'content': 'You are a helpful assistant that classifies sentiment'},\n",
    "        {'role': 'user', 'content': 'Classify the sentiment of the following sentence: I love my husband'},\n",
    "        {'role': 'assistant', 'content': 'Positive'},\n",
    "        {'role': 'user', 'content': 'Classify the sentiment of the following sentence: Kids hate sitting still'}\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
